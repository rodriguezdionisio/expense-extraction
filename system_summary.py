#!/usr/bin/env python3
"""
Script de resumen final del sistema de extracci√≥n con GCS
"""

import os
from utils.gcp import get_storage_client
from utils.env_config import config
from utils.logger import get_logger

logger = get_logger(__name__)

def system_summary():
    """Muestra un resumen completo del sistema"""
    print("üå©Ô∏è  RESUMEN FINAL DEL SISTEMA DE EXTRACCI√ìN")
    print("="*60)
    
    # Estado local
    print("\nüìÅ ESTADO LOCAL:")
    local_json = len([f for f in os.listdir("raw/") if f.startswith("expense_") and f.endswith(".json")])
    local_parquet = 0
    for root, dirs, files in os.walk("clean/"):
        for file in files:
            if file.endswith(".parquet"):
                local_parquet += 1
    
    # Contar IDs extra√≠dos
    extracted_ids = 0
    if os.path.exists("logs/extracted_expenses_log.txt"):
        with open("logs/extracted_expenses_log.txt", 'r') as f:
            extracted_ids = len([line for line in f if line.strip().isdigit()])
    
    # Contar particiones locales
    local_partitions = len([d for d in os.listdir("clean/fact_expenses/") if d.startswith("date=")])
    
    print(f"  - Archivos JSON: {local_json}")
    print(f"  - Archivos Parquet: {local_parquet}")
    print(f"  - IDs extra√≠dos en log: {extracted_ids}")
    print(f"  - Particiones de fechas: {local_partitions}")
    
    # Estado GCS
    print("\nüå©Ô∏è  ESTADO EN GOOGLE CLOUD STORAGE:")
    try:
        client = get_storage_client()
        if client:
            bucket = client.bucket(config.GCS_BUCKET_NAME)
            
            # JSON en GCS
            gcs_json = len([blob for blob in bucket.list_blobs(prefix="raw/fact_expenses/expense_") if blob.name.endswith('.json')])
            
            # Parquet en GCS
            gcs_expenses = len([blob for blob in bucket.list_blobs(prefix="clean/fact_expenses/") if blob.name.endswith('.parquet')])
            gcs_orders = len([blob for blob in bucket.list_blobs(prefix="clean/fact_expense_orders/") if blob.name.endswith('.parquet')])
            
            print(f"  - Archivos JSON: {gcs_json}")
            print(f"  - Archivos fact_expenses: {gcs_expenses}")
            print(f"  - Archivos fact_expense_orders: {gcs_orders}")
            print(f"  - Bucket: gs://{config.GCS_BUCKET_NAME}")
            
            # Archivos m√°s recientes en GCS
            recent_json = []
            for blob in bucket.list_blobs(prefix="raw/fact_expenses/expense_"):
                if blob.name.endswith('.json'):
                    recent_json.append(blob.name)
            
            if recent_json:
                print(f"  - √öltimos JSON en GCS:")
                for file in sorted(recent_json)[-3:]:
                    print(f"    ‚Ä¢ {file}")
        else:
            print("  ‚ùå No se pudo conectar a GCS")
    except Exception as e:
        print(f"  ‚ùå Error consultando GCS: {e}")
    
    print("\n‚öôÔ∏è  CONFIGURACI√ìN DEL SISTEMA:")
    print(f"  - Proyecto GCP: {config.GCP_PROJECT_ID}")
    print(f"  - Credenciales: {config.GOOGLE_APPLICATION_CREDENTIALS}")
    
    print("\nüöÄ FUNCIONALIDADES DISPONIBLES:")
    print("  ‚úÖ Extracci√≥n desde API Fudo")
    print("  ‚úÖ Almacenamiento local (JSON + Parquet)")
    print("  ‚úÖ Almacenamiento en Google Cloud Storage")
    print("  ‚úÖ Particionado por fechas")
    print("  ‚úÖ Logging de duplicados")
    print("  ‚úÖ Formato Parquet optimizado")
    print("  ‚úÖ Estructura data warehouse")
    print("  ‚úÖ Sincronizaci√≥n autom√°tica a GCS")
    print("  ‚úÖ Orquestador automatizado")
    
    print("\nüéØ COMANDOS PRINCIPALES:")
    print("  python main.py auto                           # Pr√≥ximo lote autom√°tico")
    print("  python main.py range <start_id> <end_id>      # Procesar rango espec√≠fico")
    print("  python main.py continuous                     # Procesamiento continuo")
    print("  python verify_expense_gcs.py                  # Verificar GCS")
    
    print(f"\n‚úÖ SISTEMA LISTO PARA PRODUCCI√ìN")

if __name__ == "__main__":
    system_summary()
