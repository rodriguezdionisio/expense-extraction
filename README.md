# Fudo Data Extraction Pipeline

Pipeline para extraer y procesar datos de la API de Fudo, dise√±ado para almacenamiento en data lake con **particionado por fecha**.

## üèóÔ∏è Arquitectura

```
Raw Data por Fecha (JSON) ‚Üí Processing ‚Üí Clean CSV Files
        ‚Üì                     ‚Üì              ‚Üì
   expenses_by_date/    src/process_    processed_data/
                        expenses.py
```

## üìÅ Estructura de Archivos

```
expense-extraction/
‚îú‚îÄ‚îÄ main.py                  # Orquestador principal
‚îú‚îÄ‚îÄ src/                     # M√≥dulos principales
‚îÇ   ‚îú‚îÄ‚îÄ extract_expenses.py  # Extractor de datos raw en JSON
‚îÇ   ‚îî‚îÄ‚îÄ process_expenses.py  # Procesador JSON ‚Üí CSV
‚îú‚îÄ‚îÄ raw_data/                # Datos raw en JSON
‚îÇ   ‚îú‚îÄ‚îÄ expenses/
‚îÇ   ‚îú‚îÄ‚îÄ payment-methods/
‚îÇ   ‚îî‚îÄ‚îÄ cash-registers/
‚îú‚îÄ‚îÄ processed_data/          # Datos procesados en CSV
‚îú‚îÄ‚îÄ utils/                   # Utilidades
‚îî‚îÄ‚îÄ config/                  # Configuraci√≥n
```

## üöÄ Uso Principal: Extracci√≥n por Fecha (recomendado)

### Extracci√≥n Completa
```bash
# Extraer todos los datos particionados por D√çA (m√°xima granularidad)
python main.py --mode extract-by-date --partition-by day

# Extraer todos los datos particionados por MES (balance √≥ptimo)
python main.py --mode extract-by-date --partition-by month

# Extraer todos los datos particionados por A√ëO (agregado)
python main.py --mode extract-by-date --partition-by year
```

### Extracci√≥n de Per√≠odos Espec√≠ficos
```bash
# Extraer datos de un per√≠odo espec√≠fico por D√çA
python main.py --mode extract-by-date --start-date 2025-01-01 --end-date 2025-12-31 --partition-by day

# Extraer solo los √∫ltimos 30 d√≠as
python main.py --mode extract-by-date --start-date 2025-07-01 --end-date 2025-07-30 --partition-by day

# Extraer datos anuales
python main.py --mode extract-by-date --start-date 2024-01-01 --end-date 2024-12-31 --partition-by year
```

### Procesamiento de Datos
```bash
# Procesar datos JSON extra√≠dos a CSV
python main.py --mode process
```

### Ejecuci√≥n Individual de M√≥dulos
```bash
# Solo extracci√≥n (usar los m√≥dulos directamente)
python src/extract_expenses.py

# Solo procesamiento
python src/process_expenses.py
```

## üìä Outputs

### Raw Data (JSON) - Particionado por Fecha
- `raw_data/expenses_by_date/expenses_day_YYYY-MM-DD_TIMESTAMP.json` - Archivos por d√≠a
- `raw_data/expenses_by_date/expenses_month_YYYY-MM_TIMESTAMP.json` - Archivos por mes  
- `raw_data/expenses_by_date/expenses_year_YYYY_TIMESTAMP.json` - Archivos por a√±o
- `raw_data/expenses_by_date/extraction_metadata_PARTITION_TIMESTAMP.json` - Metadatos de extracci√≥n

### Processed Data (CSV)
- `processed_data/expenses_flattened_TIMESTAMP.csv` - Datos completos aplanados (archivo principal)
- `processed_data/expense_items_relationships_TIMESTAMP.csv` - Relaciones de items
- `processed_data/processing_report_expenses_TIMESTAMP.json` - Reporte de calidad

## üéØ Ventajas de Extracci√≥n por Fecha

### ‚úÖ Para Data Lake
- **Particionado natural**: Archivos organizados por fecha (a√±o/mes/d√≠a)
- **Consultas eficientes**: Acceso directo a per√≠odos espec√≠ficos
- **Paralelizaci√≥n**: Procesamiento independiente por partici√≥n
- **Escalabilidad**: Ideal para vol√∫menes grandes de datos

### ‚úÖ Para An√°lisis
- **Actualizaciones incrementales**: Solo extraer nuevos datos
- **An√°lisis temporal**: Comparaci√≥n entre per√≠odos
- **Menor transferencia**: Descargar solo el rango necesario
- **Recuperaci√≥n granular**: Reextraer solo fechas espec√≠ficas con errores

### üìà Recomendaciones de Uso
- **Data Lake/S3**: Usar `--partition-by month` para balance √≥ptimo
- **An√°lisis diario**: Usar `--partition-by day` para granularidad m√°xima  
- **Reportes anuales**: Usar `--partition-by year` para agregaci√≥n
- **Actualizaciones**: Especificar `--start-date` y `--end-date` para per√≠odos espec√≠ficos

## üîß Configuraci√≥n

### Variables de Entorno
```bash
GCP_PROJECT_ID=your_project_id
GOOGLE_APPLICATION_CREDENTIALS=config/credentials.json
```

### Secrets en GCP
- `fudo-api-key`: API Key de Fudo
- `fudo-api-secret`: API Secret de Fudo

## üìã Campos Extra√≠dos

### Gastos (Expenses)
- **B√°sicos**: ID, monto, fecha, descripci√≥n, estado cancelado
- **Timestamps**: fecha creaci√≥n, fecha vencimiento
- **Relaciones**: 
  - Caja registradora (ID/tipo)
  - M√©todo de pago (ID/tipo)
  - Categor√≠a de gasto (ID/tipo)
  - Items de gasto (IDs/tipos/cantidad)

### Estructura de Datos
```json
{
  "id": "123",
  "type": "Expense",
  "attributes": {
    "amount": 1500.0,
    "date": "2025-01-15",
    "description": "Compra de materiales"
  },
  "relationships": {
    "paymentMethod": {"data": {"id": "1", "type": "PaymentMethod"}},
    "expenseItems": {"data": [{"id": "456", "type": "ExpenseItem"}]}
  }
}
```

## üîÑ Procesamiento

### Aplanado de Datos
- `attributes` ‚Üí campos con prefijo `attr_`
- `relationships` ‚Üí campos con prefijo `rel_`
- Listas ‚Üí campos `_count`, `_ids`, `_types`

### Ejemplo de Transformaci√≥n
```
JSON: {"attributes": {"amount": 1500}, "relationships": {"paymentMethod": {"data": {"id": "1"}}}}
CSV:  attr_amount=1500, rel_paymentMethod_id=1
```

## üìà An√°lisis Incluidos

### Datos Generados
- **Flattened CSV**: Datos completos aplanados con todos los campos y relaciones
- **Relationships CSV**: Mapeo detallado de gastos ‚Üí items de gastos
- **Calidad de datos**: Reporte JSON con estad√≠sticas y validaciones

### Archivos Especializados
- **Flattened CSV**: Archivo principal con todos los datos normalizados
- **Relationships CSV**: Mapeo de gastos ‚Üí items para an√°lisis relacionales
- **Processing Report**: M√©tricas de calidad y estad√≠sticas de procesamiento

## üõ†Ô∏è Mantenimiento

### Logs
Todos los procesos generan logs detallados usando el sistema de logging configurado.

### Recuperaci√≥n de Errores
- Extracci√≥n por p√°ginas permite reanudar desde cualquier punto
- Metadatos de extracci√≥n incluyen informaci√≥n de recuperaci√≥n
- Procesamiento valida archivos JSON antes de procesar

### Monitoreo
- Reportes de calidad de datos en JSON
- Conteo de registros en cada fase
- Timestamps para auditor√≠a

## üîç Troubleshooting

### Error de Autenticaci√≥n
```bash
# Verificar secrets
python -c "from utils.gcp import get_secret; print(get_secret('fudo-api-key'))"
```

### Error de Datos Faltantes
```bash
# Verificar archivos raw
ls -la raw_data/expenses/
```

### Error de Procesamiento
```bash
# Ejecutar solo procesamiento con logs
python src/process_expenses.py
```

## üìö Ejemplos de Uso Pr√°cticos

### Extracci√≥n Inicial Completa
```bash
# Extraer TODOS los datos disponibles particionados por d√≠a
python main.py --mode extract-by-date --partition-by day
```

### Actualizaci√≥n Incremental Diaria
```bash
# Extraer solo los datos de hoy
python main.py --mode extract-by-date --start-date 2025-07-30 --end-date 2025-07-30 --partition-by day

# Extraer √∫ltimos 7 d√≠as
python main.py --mode extract-by-date --start-date 2025-07-23 --end-date 2025-07-30 --partition-by day
```

### Data Lake con Python
```python
from src.extract_expenses import FudoRawExtractor

extractor = FudoRawExtractor(output_dir="s3://my-data-lake/raw/fudo/")
extractor.get_token()

# Extracci√≥n particionada por mes para data lake
result = extractor.extract_expenses_by_date(
    start_date="2025-01-01",
    end_date="2025-12-31", 
    partition_by="month"
)
```

### Procesamiento de Datos
```python
from src.process_expenses import FudoDataProcessor

processor = FudoDataProcessor(
    raw_data_dir="raw_data/expenses_by_date/",
    output_dir="processed_data/"
)
processor.process_expenses()
```

---

**Nota**: Este pipeline est√° dise√±ado para ser modular y escalable, permitiendo procesamiento tanto local como en cloud.
