# ğŸ“Š Expense Extraction & Processing System

Sistema automatizado de extracciÃ³n y procesamiento de gastos desde la API de Fudo con transformaciÃ³n a Parquet estructurado, orquestaciÃ³n completa y sincronizaciÃ³n en Google Cloud Storage.

## DescripciÃ³n

Este proyecto extrae datos de gastos (expenses) desde la API REST de Fudo, los procesa y los convierte en archivos Parquet particionados por fecha para anÃ¡lisis de datos. El sistema incluye un orquestador automatizado (`main.py`) que controla todo el pipeline, sistema de logging para prevenir duplicados, y estructura de datos tipo data warehouse con formato Parquet de alto rendimiento.

## CaracterÃ­sticas Principales

- **ğŸ“Š ExtracciÃ³n Inteligente**: Sistema con logging para prevenir duplicados automÃ¡ticamente
- **ğŸ—ï¸ Arquitectura Simplificada**: CÃ³digo optimizado y limpio
- **ğŸ“ Estructura Data Warehouse**: Tablas fact separadas (`fact_expenses`, `fact_expense_orders`)
- **ğŸ—‚ï¸ Particionado Hive-Style**: Estructura `clean/fact_*/date=YYYY-MM-DD/`
- **ğŸš€ Formato Parquet Optimizado**: Alto rendimiento con compresiÃ³n y tipado
- **ğŸ” AutenticaciÃ³n Segura**: IntegraciÃ³n con Google Cloud Secret Manager
- **ğŸ¤– Orquestador Automatizado**: `main.py` controla todo el pipeline
- **ğŸŒ©ï¸ SincronizaciÃ³n GCS**: Almacenamiento automÃ¡tico en la nube
- **ğŸ—ï¸ Arquitectura Simplificada**: CÃ³digo optimizado y limpio
- **ğŸ“ Estructura Data Warehouse**: Tablas fact separadas (`fact_expenses`, `fact_expense_orders`)
- **ğŸ—‚ï¸ Particionado Hive-Style**: Estructura `clean/fact_*/date=YYYY-MM-DD/`
- **ğŸš€ Formato Parquet Optimizado**: Alto rendimiento con compresiÃ³n y tipado
- **ğŸ” AutenticaciÃ³n Segura**: IntegraciÃ³n con Google Cloud Secret Manager
- **ğŸ¤– Orquestador Automatizado**: `main.py` controla todo el pipeline
- **ğŸŒ©ï¸ SincronizaciÃ³n GCS**: Almacenamiento automÃ¡tico en la nubeiÃ³n Inteligente**: Sistema con logging para prevenir duplicados automÃ¡ticamente
- **ğŸ—ï¸ Arquitectura Simplificada**: CÃ³digo optimizado reducido en 28% manteniendo funcionalidad completa
- **ğŸ“ Estructura Data Warehouse**: Tablas fact separadas (`fact_expenses`, `fact_expense_orders`)
- **ğŸ—‚ï¸ Particionado Hive-Style**: Estructura `clean/fact_*/date=YYYY-MM-DD/`
- **ğŸš€ Formato Parquet**: Archivos de alto rendimiento con compresiÃ³n y tipado optimizado
- **ğŸ” AutenticaciÃ³n Segura**: IntegraciÃ³n con Google Cloud Secret Manager
- **ğŸ“ Sistema de Logging**: PrevenciÃ³n automÃ¡tica de extracciones duplicadas
- **ğŸ› ï¸ Scripts CLI Simplificados**: Interfaces de lÃ­nea de comandos fÃ¡ciles de usarion & Processing System

Sistema optimizado de extracciÃ³n y procesamiento de gastos desde la API de Fudo con transformaciÃ³n a Parquet estructurado y prevenciÃ³n de duplicados.

## DescripciÃ³n

Este proyecto extrae datos de gastos (expenses) desde la API REST de Fudo, los procesa y los convierte en archivos Parquet particionados por fecha para anÃ¡lisis de datos. El sistema ha sido completamente optimizado con arquitectura simplificada, sistema de logging para prevenir duplicados, y estructura de datos tipo data warehouse con formato Parquet de alto rendimiento.

## CaracterÃ­sticas Principales

- **ï¿½ ExtracciÃ³n Inteligente**: Sistema con logging para prevenir duplicados automÃ¡ticamente
- **ğŸ—ï¸ Arquitectura Simplificada**: CÃ³digo optimizado reducido en 28% manteniendo funcionalidad completa
- **ğŸ“ Estructura Data Warehouse**: Tablas fact separadas (`fact_expenses`, `fact_expense_orders`)
- **ï¿½ï¸ Particionado Hive-Style**: Estructura `clean/fact_*/date=YYYY-MM-DD/`
- **ğŸ” AutenticaciÃ³n Segura**: IntegraciÃ³n con Google Cloud Secret Manager
- **ï¿½ Sistema de Logging**: PrevenciÃ³n automÃ¡tica de extracciones duplicadas
- **ğŸš€ Scripts CLI Simplificados**: Interfaces de lÃ­nea de comandos fÃ¡ciles de usar

## Estructura del Proyecto

```
expense-extraction/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ credentials.json          # Credenciales de Google Cloud
â”œâ”€â”€ utils/                        # MÃ³dulos de utilidades optimizados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ env_config.py            # ConfiguraciÃ³n de variables de entorno
â”‚   â”œâ”€â”€ gcp.py                   # Utilidades de Google Cloud
â”‚   â””â”€â”€ logger.py                # Sistema de logging
â”œâ”€â”€ raw/                         # Archivos JSON individuales extraÃ­dos
â”‚   â”œâ”€â”€ expense_1.json
â”‚   â”œâ”€â”€ expense_2.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ clean/                       # Datos procesados en estructura data warehouse
â”‚   â”œâ”€â”€ fact_expenses/           # Tabla principal de gastos
â”‚   â”‚   â”œâ”€â”€ date=2019-10-27/
â”‚   â”‚   â”‚   â””â”€â”€ fact_expenses.parquet
â”‚   â”‚   â”œâ”€â”€ date=2020-01-04/
â”‚   â”‚   â”‚   â””â”€â”€ fact_expenses.parquet
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ fact_expense_orders/     # Tabla de Ã³rdenes/items de gastos
â”‚       â”œâ”€â”€ date=2019-10-27/
â”‚       â”‚   â””â”€â”€ fact_expense_orders.parquet
â”‚       â”œâ”€â”€ date=2020-01-04/
â”‚       â”‚   â””â”€â”€ fact_expense_orders.parquet
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logs/                        # Sistema de logging para duplicados
â”‚   â””â”€â”€ extracted_expenses_log.txt
â”œâ”€â”€ main.py                     # ğŸ¯ ORQUESTADOR PRINCIPAL
â”œâ”€â”€ expense_extractor.py         # Sistema de extracciÃ³n optimizado
â”œâ”€â”€ expense_processor.py         # Sistema de procesamiento optimizado
â”œâ”€â”€ system_summary.py           # Resumen del sistema
â”œâ”€â”€ verify_expense_gcs.py       # VerificaciÃ³n de GCS
â”œâ”€â”€ requirements.txt            # Dependencias
â””â”€â”€ README.md                   # Este archivo
```

## Arquitectura del Sistema

```
ğŸ“ expense-extraction/
â”œâ”€â”€ main.py                     # ğŸ¯ Orquestador principal (USAR ESTE)
â”œâ”€â”€ expense_extractor.py        # ExtracciÃ³n de datos desde API
â”œâ”€â”€ expense_processor.py        # Procesamiento y conversiÃ³n Parquet
â”œâ”€â”€ system_summary.py           # Resumen del estado del sistema
â”œâ”€â”€ config/
â”‚   â””â”€â”€ credentials.json        # Credenciales GCP
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ env_config.py          # ConfiguraciÃ³n de environment
â”‚   â”œâ”€â”€ gcp.py                 # Utilidades Google Cloud Storage
â”‚   â””â”€â”€ logger.py              # Sistema de logging unificado
â””â”€â”€ requirements.txt           # Dependencias Python
```

**Archivos principales optimizados**:
- **main.py** (311 lÃ­neas): Interfaz CLI unificada
- **expense_extractor.py** (278 lÃ­neas): ExtracciÃ³n desde Fudo API  
- **expense_processor.py** (296 lÃ­neas): ConversiÃ³n a formato Parquet
- **utils/gcp.py** (121 lÃ­neas): Operaciones Google Cloud Storage

## InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone <repository-url>
cd expense-extraction
```

2. **Crear entorno virtual**:
```bash
python -m venv .venv
source .venv/bin/activate  # En Linux/Mac
# .venv\Scripts\activate  # En Windows
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno**:
```bash
cp .env.example .env
# Editar .env con tus valores
```

5. **Configurar credenciales de Google Cloud**:
   - Crear proyecto en Google Cloud
   - Habilitar Secret Manager API
   - Crear service account y descargar credenciales
   - Colocar credenciales en `config/credentials.json`

## ConfiguraciÃ³n

### Variables de Entorno (.env)

```env
# Google Cloud
GCP_PROJECT_ID=tu-proyecto-gcp-id
GCP_PROJECT_NAME=tu-proyecto-gcp
GCS_BUCKET_NAME=tu-bucket-name
GOOGLE_APPLICATION_CREDENTIALS=config/credentials.json

# ConfiguraciÃ³n para ambiente
ENV=local
```

### Secretos en Google Cloud Secret Manager

El sistema requiere los siguientes secretos configurados en Secret Manager:
- **`fudo-api-key`**: Tu API key de Fudo
- **`fudo-api-secret`**: Tu API secret de Fudo

## Uso del Sistema

### ğŸ¯ Orquestador Principal (Recomendado)

El nuevo archivo `main.py` automatiza todo el flujo de extracciÃ³n, procesamiento y almacenamiento:

#### Procesamiento AutomÃ¡tico del PrÃ³ximo Lote
```bash
# Procesa automÃ¡ticamente los prÃ³ximos 10 IDs desde donde se quedÃ³
python main.py auto

# Procesa automÃ¡ticamente lote de tamaÃ±o personalizado
python main.py auto --batch-size 20
```

#### Procesamiento de Rango EspecÃ­fico
```bash
# Pipeline completo: extracciÃ³n + procesamiento + almacenamiento
python main.py range 1 20

# Un solo expense
python main.py range 25 25
```

#### Procesamiento Continuo Automatizado
```bash
# Procesamiento continuo con lotes de 10, pausa de 60 segundos
python main.py continuous

# Procesamiento continuo personalizado
python main.py continuous --batch-size 20 --delay 30 --max-batches 5
```

#### Operaciones Individuales
```bash
# Solo extracciÃ³n (sin procesamiento)
python main.py extract 1 20

# Solo procesamiento (sin extracciÃ³n)
python main.py process 1 20
```

## Estructura de Archivos de Salida

### Estructura Data Warehouse

El sistema genera dos tablas fact separadas organizadas por fechas:

```
clean/
â”œâ”€â”€ fact_expenses/              # Tabla principal de gastos
â”‚   â”œâ”€â”€ date=2019-10-27/
â”‚   â”‚   â””â”€â”€ fact_expenses.csv
â”‚   â”œâ”€â”€ date=2020-01-04/
â”‚   â”‚   â””â”€â”€ fact_expenses.csv
â”‚   â””â”€â”€ ...
â””â”€â”€ fact_expense_orders/        # Tabla de Ã³rdenes/items
    â”œâ”€â”€ date=2019-10-27/
    â”‚   â””â”€â”€ fact_expense_orders.parquet
    â”œâ”€â”€ date=2020-01-04/
    â”‚   â””â”€â”€ fact_expense_orders.parquet
    â””â”€â”€ ...
```

### Archivos Parquet Generados

#### `fact_expenses.parquet` - Datos principales de gastos
Columnas optimizadas con tipos de datos eficientes:
- `expense_key` (int64): Clave primaria del gasto
- `expense_amount` (float64): Monto del gasto
- `cancelled` (bool): Estado de cancelaciÃ³n
- `expense_date_key` (int64): Fecha del gasto (YYYYMMDD)
- `created_date_key`, `created_time_key` (int64): Fechas y hora de creaciÃ³n
- Claves forÃ¡neas: `cashregister_key`, `paymentmethod_key`, `provider_key`, etc.

#### `fact_expense_orders.parquet` - LÃ­neas de detalle (expense items)
Columnas optimizadas para anÃ¡lisis de items:
- `expense_order_key` (int64): Clave primaria del item
- `expense_key` (int64): Clave forÃ¡nea al expense principal
- `item_detail` (string): DescripciÃ³n del item
- `item_price`, `item_quantity` (float64): Precio y cantidad
- Datos de productos e ingredientes con prefijos optimizados

## Transformaciones Aplicadas

### Renombrado de Columnas
- `expense_id` â†’ `expense_key` (int64)
- `amount` â†’ `expense_amount` (float64)
- `canceled` â†’ `cancelled` (bool)
- `date` â†’ `expense_date_key` (int64, formato YYYYMMDD)
- `created_at` â†’ `created_date_key`, `created_time_key` (int64)
- `description` â†’ `expense_note` (string)
- IDs de relaciones â†’ `*_key` (int64)

### TransformaciÃ³n de Expense Items
- `expense_item_id` â†’ `expense_order_key` (int64)
- `expense_id` â†’ `expense_key` (int64, clave forÃ¡nea)
- `detail` â†’ `item_detail` (string)
- `price` â†’ `item_price` (float64)
- `quantity` â†’ `item_quantity` (float64)
- Datos de productos e ingredientes incluidos con prefijos

### Sistema de Logging
- **PrevenciÃ³n de Duplicados**: `logs/extracted_expenses_log.txt`
- **VerificaciÃ³n AutomÃ¡tica**: El sistema verifica IDs ya extraÃ­dos
- **InicializaciÃ³n Inteligente**: Detecta archivos existentes al inicio

### Particionado por Fecha

Los archivos se organizan en estructura Hive-style por tablas fact:
```
clean/
â”œâ”€â”€ fact_expenses/
â”‚   â”œâ”€â”€ date=2019-10-27/
â”‚   â”‚   â””â”€â”€ fact_expenses.parquet      # Expenses de esa fecha
â”‚   â”œâ”€â”€ date=2020-01-04/
â”‚   â”‚   â””â”€â”€ fact_expenses.parquet
â”‚   â””â”€â”€ ...
â””â”€â”€ fact_expense_orders/
    â”œâ”€â”€ date=2019-10-27/
    â”‚   â””â”€â”€ fact_expense_orders.parquet # Items de esa fecha
    â”œâ”€â”€ date=2020-01-04/
    â”‚   â””â”€â”€ fact_expense_orders.parquet
    â””â”€â”€ ...
```

### Archivos Parquet Generados

#### `fact_expenses.parquet` - Datos principales de gastos
Columnas optimizadas con tipos de datos eficientes:
- `expense_key` (int64): Clave primaria del gasto
- `expense_amount` (float64): Monto del gasto
- `cancelled` (bool): Estado de cancelaciÃ³n
- `expense_date_key` (int64): Fecha del gasto (YYYYMMDD)
- `payment_date_key`, `due_date_key`, `created_date_key` (int64): Fechas relacionadas
- `created_time_key` (int64): Hora de creaciÃ³n (HHMM)
- Claves forÃ¡neas: `cashregister_key`, `paymentmethod_key`, `provider_key`, etc.

#### `fact_expense_orders.parquet` - LÃ­neas de detalle (expense items)
Columnas optimizadas para anÃ¡lisis de items:
- `expense_order_key` (int64): Clave primaria del item
- `expense_key` (int64): Clave forÃ¡nea al expense principal
- `item_detail` (string): DescripciÃ³n del item
- `item_price`, `item_quantity` (float64): Precio y cantidad
- Datos de productos e ingredientes con prefijos optimizados

## API Reference

### ExpenseExtractor (Optimizado)
```python
from expense_extractor import ExpenseExtractor

extractor = ExpenseExtractor()

# Inicializar sistema de logging
extractor.initialize_log_from_existing_files()

# Extraer rango con prevenciÃ³n de duplicados
expenses, count = extractor.extract_range(1, 20)
```

### ExpenseProcessor (Optimizado)
```python
from expense_processor import ExpenseProcessor

processor = ExpenseProcessor()

# Procesar rango especÃ­fico
processor.process_range(1, 20)
```

## Optimizaciones del Sistema

- **ğŸ—ï¸ Arquitectura Simplificada**: EliminaciÃ³n de scripts redundantes y cÃ³digo duplicado
- **ğŸ¯ Orquestador Centralizado**: `main.py` controla todo el pipeline automatizado
- **ğŸ“‹ Sistema de Logging**: PrevenciÃ³n automÃ¡tica de duplicados con seguimiento de IDs
- **ğŸ”§ ValidaciÃ³n Unificada**: Funciones reutilizables para validaciÃ³n de rangos
- **ğŸ“š DocumentaciÃ³n Actualizada**: README coherente con estado actual
- **ğŸš€ Formato Parquet Optimizado**: 
  - CompresiÃ³n automÃ¡tica para menor uso de espacio
  - Tipado de datos optimizado para consultas rÃ¡pidas
  - Compatible con herramientas de anÃ¡lisis modernas (Pandas, Spark, etc.)
  - Metadatos incluidos para mejor rendimiento
  - SincronizaciÃ³n automÃ¡tica con Google Cloud Storage

## ğŸ¯ Orquestador Principal (main.py)

El orquestador `main.py` es la **interfaz principal** del sistema. Automatiza completamente el flujo de extracciÃ³n, procesamiento y almacenamiento, reemplazando los scripts manuales anteriores.

### CaracterÃ­sticas del Orquestador:

- **ğŸ¤– AutomatizaciÃ³n Completa**: Un solo comando ejecuta todo el pipeline
- **ğŸ“Š DetecciÃ³n Inteligente**: Identifica automÃ¡ticamente el prÃ³ximo lote a procesar
- **ğŸ”„ Procesamiento Continuo**: Soporte para ejecuciÃ³n tipo daemon
- **âš ï¸ Manejo de Errores**: Control robusto de errores y logging detallado
- **ğŸ›ï¸ Flexibilidad**: MÃºltiples modos de operaciÃ³n segÃºn necesidades

### Comandos Principales:

```bash
# ğŸš€ RECOMENDADO: Procesamiento automÃ¡tico
python main.py auto                    # PrÃ³ximos 10 IDs automÃ¡ticamente
python main.py auto --batch-size 20    # Lote personalizado

# ğŸ“Š Rango especÃ­fico  
python main.py range 1 50              # Pipeline completo para IDs 1-50

# ğŸ”„ Procesamiento continuo (producciÃ³n)
python main.py continuous              # Lotes de 10, pausa 60 segundos
python main.py continuous --batch-size 15 --delay 30 --max-batches 5

# âš™ï¸ Operaciones individuales
python main.py extract 1 100           # Solo extracciÃ³n
python main.py process 1 100           # Solo procesamiento
```

### Casos de Uso del Orquestador:

1. **Procesamiento Inicial Masivo**:
   ```bash
   python main.py range 1 1000
   ```

2. **Mantenimiento Diario Automatizado**:
   ```bash
   python main.py auto --batch-size 50
   ```

3. **Procesamiento Continuo (Servidor)**:
   ```bash
   python main.py continuous --batch-size 20 --delay 300
   ```

4. **Testing y Desarrollo**:
   ```bash
   python main.py continuous --batch-size 3 --delay 10 --max-batches 2
   ```

## Ventajas del Formato Parquet

- **ğŸš€ Rendimiento**: Consultas hasta 10x mÃ¡s rÃ¡pidas que CSV
- **ğŸ’¾ CompresiÃ³n**: Archivos 50-80% mÃ¡s pequeÃ±os
- **ğŸ¯ Tipado**: PreservaciÃ³n de tipos de datos (int64, float64, bool, string)
- **ğŸ“Š Compatibilidad**: Soporte nativo en Pandas, Spark, PowerBI, Tableau
- **ğŸ” Filtrado**: Pushdown predicates para consultas eficientes
- **ğŸ“ˆ Columnar**: Almacenamiento optimizado para anÃ¡lisis

## Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.
